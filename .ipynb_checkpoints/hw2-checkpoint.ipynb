{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b7a7fa",
   "metadata": {},
   "source": [
    "# Homework 2: Considering Bias in Data\n",
    "\n",
    "In this project, we will be looking at the wikipedia articles of politicians from around the world. We will be combining data sets containing country/region populations and politician article quality to generate some analysis about the quality of articles pertaining to each country."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895dab85",
   "metadata": {},
   "source": [
    "# Step 1: Getting the Article and Population Data\n",
    "\n",
    "This data has already been collected for us and can be found in politicians_by_country_AUG.2024.csv and population_by_country_AUG.2024.csv. The first file contains a list of politicians, their article link, and their country while the second file contains a list of countries/regions and their respective populations (in millions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a43516e",
   "metadata": {},
   "source": [
    "# Step 2: Getting Article Quality Predictions\n",
    "\n",
    "In this step, we will use the ORAS machine learning system to get quality predictions on each politician's article. To do this, we first need to get the most recent revision of each article, denoted by a revision id. Then we pass the article name and the revision id to the ORAS system to get a quality prediction. We will also keep track of all articles that are unable to be given a quality score for whatever reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "504b7e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# These are standard python modules\n",
    "import json, time, urllib.parse\n",
    "#\n",
    "# The 'requests' module is not a standard Python module. You will need to install this with pip/pip3 if you do not already have it\n",
    "import requests\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893699b6",
   "metadata": {},
   "source": [
    "The following code block is from the wp_page_info_example notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90948598",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The basic English Wikipedia API endpoint\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "API_HEADER_AGENT = 'User-Agent'\n",
    "\n",
    "# We'll assume that there needs to be some throttling for these requests - we should always be nice to a free data resource\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': 'ashwin19@uw.edu, University of Washington, MSDS DATA 512 - AUTUMN 2024'\n",
    "}\n",
    "\n",
    "# This is just a list of English Wikipedia article titles that we can use for example requests\n",
    "ARTICLE_TITLES = [ 'Bison', 'Northern flicker', 'Red squirrel', 'Chinook salmon', 'Horseshoe bat' ]\n",
    "\n",
    "# This is a string of additional page properties that can be returned see the Info documentation for\n",
    "# what can be included. If you don't want any this can simply be the empty string\n",
    "# PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"\"\n",
    "\n",
    "# This template lists the basic parameters for making this\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",           # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a368abd4",
   "metadata": {},
   "source": [
    "The following code block is also from the wp_page_info_example notebook and defines a function to make the API call to the page info API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48e1f604",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_pageinfo_per_article(article_title = None, \n",
    "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                 headers = REQUEST_HEADERS):\n",
    "    \n",
    "    # article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['titles'] = article_title\n",
    "\n",
    "    if not request_template['titles']:\n",
    "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\n",
    "\n",
    "    if API_HEADER_AGENT not in headers:\n",
    "        raise Exception(f\"The header data should include a '{API_HEADER_AGENT}' field that contains your UW email address.\")\n",
    "\n",
    "    if 'uwnetid@uw' in headers[API_HEADER_AGENT]:\n",
    "        raise Exception(f\"Use your UW email address in the '{API_HEADER_AGENT}' field.\")\n",
    "\n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or any other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fc7e7d",
   "metadata": {},
   "source": [
    "The following code block defines constants used for making ORES API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c4e1ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "#    The current LiftWing ORES API endpoint and prediction model\n",
    "#\n",
    "API_ORES_LIFTWING_ENDPOINT = \"https://api.wikimedia.org/service/lw/inference/v1/models/{model_name}:predict\"\n",
    "API_ORES_EN_QUALITY_MODEL = \"enwiki-articlequality\"\n",
    "\n",
    "#\n",
    "#    The throttling rate is a function of the Access token that you are granted when you request the token. The constants\n",
    "#    come from dissecting the token and getting the rate limits from the granted token. An example of that is below.\n",
    "#\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = ((60.0*60.0)/5000.0)-API_LATENCY_ASSUMED  # The key authorizes 5000 requests per hour\n",
    "\n",
    "#    When making automated requests we should include something that is unique to the person making the request\n",
    "#    This should include an email - your UW email would be good to put in there\n",
    "#    \n",
    "#    Because all LiftWing API requests require some form of authentication, you need to provide your access token\n",
    "#    as part of the header too\n",
    "#\n",
    "REQUEST_HEADER_TEMPLATE = {\n",
    "    'User-Agent': \"<{email_address}>, University of Washington, MSDS DATA 512 - AUTUMN 2024\",\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': \"Bearer {access_token}\"\n",
    "}\n",
    "#\n",
    "#    This is a template for the parameters that we need to supply in the headers of an API request\n",
    "#\n",
    "REQUEST_HEADER_PARAMS_TEMPLATE = {\n",
    "    'email_address' : \"\",         # your email address should go here\n",
    "    'access_token'  : \"\"          # the access token you create will need to go here\n",
    "}\n",
    "\n",
    "#\n",
    "#    A dictionary of English Wikipedia article titles (keys) and sample revision IDs that can be used for this ORES scoring example\n",
    "#\n",
    "ARTICLE_REVISIONS = { 'Bison':1085687913 , 'Northern flicker':1086582504 , 'Red squirrel':1083787665 , 'Chinook salmon':1085406228 , 'Horseshoe bat':1060601936 }\n",
    "\n",
    "#\n",
    "#    This is a template of the data required as a payload when making a scoring request of the ORES model\n",
    "#\n",
    "ORES_REQUEST_DATA_TEMPLATE = {\n",
    "    \"lang\":        \"en\",     # required that its english - we're scoring English Wikipedia revisions\n",
    "    \"rev_id\":      \"\",       # this request requires a revision id\n",
    "    \"features\":    True\n",
    "}\n",
    "\n",
    "#\n",
    "#    These are used later - defined here so they, at least, have empty values\n",
    "#\n",
    "USERNAME = \"\"\n",
    "ACCESS_TOKEN = \"\"\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41ea82d",
   "metadata": {},
   "source": [
    "The following cell needs to be updated with Wikipedia API credentials!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e77dba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your credentials into this code block and uncomment it, otherwise the API calls cannot be made\n",
    "\n",
    "# USERNAME = <Put username here>\n",
    "# ACCESS_TOKEN = <Put access token here>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b419355",
   "metadata": {},
   "source": [
    "The following code block defines the function that makes the ORES API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91b7f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_ores_score_per_article(article_revid = None, email_address=None, access_token=None,\n",
    "                                   endpoint_url = API_ORES_LIFTWING_ENDPOINT, \n",
    "                                   model_name = API_ORES_EN_QUALITY_MODEL, \n",
    "                                   request_data = ORES_REQUEST_DATA_TEMPLATE, \n",
    "                                   header_format = REQUEST_HEADER_TEMPLATE, \n",
    "                                   header_params = REQUEST_HEADER_PARAMS_TEMPLATE):\n",
    "    \n",
    "    #    Make sure we have an article revision id, email and token\n",
    "    #    This approach prioritizes the parameters passed in when making the call\n",
    "    if article_revid:\n",
    "        request_data['rev_id'] = article_revid\n",
    "    if email_address:\n",
    "        header_params['email_address'] = email_address\n",
    "    if access_token:\n",
    "        header_params['access_token'] = access_token\n",
    "    \n",
    "    #   Making a request requires a revision id - an email address - and the access token\n",
    "    if not request_data['rev_id']:\n",
    "        raise Exception(\"Must provide an article revision id (rev_id) to score articles\")\n",
    "    if not header_params['email_address']:\n",
    "        raise Exception(\"Must provide an 'email_address' value\")\n",
    "    if not header_params['access_token']:\n",
    "        raise Exception(\"Must provide an 'access_token' value\")\n",
    "    \n",
    "    # Create the request URL with the specified model parameter - default is a article quality score request\n",
    "    request_url = endpoint_url.format(model_name=model_name)\n",
    "    \n",
    "    # Create a compliant request header from the template and the supplied parameters\n",
    "    headers = dict()\n",
    "    for key in header_format.keys():\n",
    "        headers[str(key)] = header_format[key].format(**header_params)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free data\n",
    "        # source like ORES - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        #response = requests.get(request_url, headers=headers)\n",
    "        response = requests.post(request_url, headers=headers, data=json.dumps(request_data))\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7249d4",
   "metadata": {},
   "source": [
    "This code block reads the politicians file, and for each article, looks up the corresponding ORES score for the most recent revision. This data is then stored into a file called article_quality_no_region.txt in csv format. \n",
    "\n",
    "NOTE: This code block takes a very long time to run so the article_quality_no_region.txt file has been already created and is ready for use in future code blocks. This code block can be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b11760d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SKIP THIS CODE BLOCK, THE OUTPUT HAS ALREADY BEEN CREATED AND IS READY FOR USE\n",
    "df_politicians = pd.read_csv('politicians_by_country_AUG.2024.csv')\n",
    "with open('article_quality_no_region.txt', 'a', encoding='utf-8') as output:\n",
    "    output.write('article_title,revision_id,article_quality,country,population,region\\n')\n",
    "    for index, row in df_politicians.iterrows():\n",
    "        name = row['name']\n",
    "        print(name)\n",
    "        country = row['country']\n",
    "\n",
    "        try:\n",
    "            # Request page data\n",
    "            info = request_pageinfo_per_article(name)['query']['pages']\n",
    "            if '-1' in info:\n",
    "                continue\n",
    "\n",
    "            rev_id = info[next(iter(info))]['lastrevid']\n",
    "            # Use the revision id to get ORES rating\n",
    "            score = request_ores_score_per_article(article_revid=rev_id,\n",
    "                   email_address=\"ashwin19@uw.edu\",\n",
    "                   access_token=ACCESS_TOKEN)\n",
    "            if 'enwiki' in score:\n",
    "                # Write output line to file\n",
    "                quality = score['enwiki']['scores'][str(rev_id)]['articlequality']['score']['prediction']\n",
    "                output.write('\"' + name + '\",' + str(rev_id) + \",\" + quality + \",\" + '\"' + country + '\",' + \",\" + \"\\n\")\n",
    "                output.flush()\n",
    "        except:\n",
    "            print(\"ERROR\")\n",
    "print('FINISHED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d0e6b0",
   "metadata": {},
   "source": [
    "The following code block computes the list of articles that were unable to be paired with an ORES quality. This is done by comparing the politicians csv file with the article_quality csv file produced in the prior code block. The displayed error rate is the ratio of the number of articles for which we were not able to get a score divided by the total number of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99151402",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Articles without ORES quality:\n",
      "187                   Abderrahmane Meziane Chérif\n",
      "430                        Barbara Eibinger-Miedl\n",
      "516                               Mehrali Gasimov\n",
      "1200                                   Kyaw Myint\n",
      "1342                       André Ngongang Ouandji\n",
      "1955                               Tomás Pimentel\n",
      "2427                                Richard Sumah\n",
      "2430    Sofoklis Avraam Choudaverdoglou-Theodotos\n",
      "2431                           Christos Daralexis\n",
      "2899                                      S. Kabo\n",
      "3088                                   Tariq Najm\n",
      "3878                      Grzegorz Antoni Ogiński\n",
      "4496                   Segun ''Aeroland'' Adewale\n",
      "5719                              Bashir Bililiqo\n",
      "Name: name, dtype: object\n",
      "\n",
      "Error Rate: 0.196 %\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('politicians_by_country_AUG.2024.csv')\n",
    "df2 = pd.read_csv('article_quality_no_region.txt')\n",
    "\n",
    "not_in_df2 = df1[~df1['name'].isin(df2['article_title'])]\n",
    "\n",
    "print(\"List of Articles without ORES quality:\")\n",
    "print(not_in_df2['name'])\n",
    "\n",
    "error_rate = round(not_in_df2.shape[0] / df1.shape[0], 5) * 100\n",
    "print(\"\\nError Rate: \" + str(error_rate) + \" %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fdd71a",
   "metadata": {},
   "source": [
    "# Step 3: Combining the Datasets\n",
    "\n",
    "In this step, we will be combining the region and populationd data with the politician data. The population and region data is found in the following file: population_by_country_AUG.2024.csv. Each politician row will get a population value and region. Countries that don't have matches will be recorded and saved in the following file: wp_countries-no_match.txt. The new data frame containing all the joined data will be saved in csv format to the following file: wp_politicians_by_country.csv. The future analysis sections rely on this joined data file which has already been created in this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6f6dab",
   "metadata": {},
   "source": [
    "The following code block parses the population file and helps keep track of which countries belong to each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeb727f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "region_map = {}\n",
    "country_map = {}\n",
    "with open('population_by_country_AUG.2024.csv', 'r') as country_file:\n",
    "    # Iterate through each line in the file\n",
    "    current_region = \"\"\n",
    "    for line in country_file:\n",
    "        if 'Geography' in line:\n",
    "            continue\n",
    "            \n",
    "        # remove white space and isolate each component\n",
    "        terms = line.strip().split(',')\n",
    "        place = terms[0]\n",
    "        pop = terms[1]\n",
    "        \n",
    "        if place.isupper(): # Region\n",
    "            current_region = place\n",
    "            region_map[place] = (pop, False)\n",
    "        else: # Country\n",
    "            country_map[place] = (pop, current_region, False)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c2f5bb",
   "metadata": {},
   "source": [
    "The following code block iterates through the data frame and adds in the corresponding region and population data. Countries without matches are stored and send to another file as output. The newly joined data frame is already written to a file as csv output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e78663b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Guinea-Bissau', 'Korean', 'Korea, South']\n",
      "['Western Sahara', 'GuineaBissau', 'Mauritius', 'Mayotte', 'Reunion', 'Sao Tome and Principe', 'eSwatini', 'Canada', 'United States', 'Mexico', 'Curacao', 'Dominica', 'Guadeloupe', 'Jamaica', 'Martinique', 'Puerto Rico', 'French Guiana', 'Suriname', 'Georgia', 'Brunei', 'Philippines', 'China (Hong Kong SAR)', 'China (Macao SAR)', 'Korea (North)', 'Korea (South)', 'Denmark', 'Iceland', 'Ireland', 'United Kingdom', 'Liechtenstein', 'Netherlands', 'Romania', 'Andorra', 'San Marino', 'Australia', 'Fiji', 'French Polynesia', 'Guam', 'Kiribati', 'Nauru', 'New Caledonia', 'New Zealand', 'Palau']\n"
     ]
    }
   ],
   "source": [
    "countries_not_mapped_1 = []\n",
    "countries_not_mapped_2 = []\n",
    "df_quality = pd.read_csv('article_quality_no_region.txt')\n",
    "\n",
    "# Iterate through the data frame and add in population and region data\n",
    "for index, row in df_quality.iterrows():\n",
    "    country = row['country']\n",
    "    if country in country_map:\n",
    "        mapping = country_map[country]\n",
    "        df_quality.loc[index, 'population'] = mapping[0]\n",
    "        df_quality.loc[index, 'region'] = mapping[1]\n",
    "        country_map[country] = (mapping[0], mapping[1], True)\n",
    "    else:\n",
    "        # Keep track of countries with no matches\n",
    "        if country not in countries_not_mapped_1:\n",
    "            countries_not_mapped_1.append(country)\n",
    "\n",
    "# Keep track of countries in the dictionary with no matches\n",
    "for key in country_map:\n",
    "    if not country_map[key][2] and key not in countries_not_mapped_2:\n",
    "        countries_not_mapped_2.append(key)\n",
    "    \n",
    "# Print and write the non-matches to a file\n",
    "print(countries_not_mapped_1)\n",
    "print(countries_not_mapped_2)\n",
    "\n",
    "with open('wp_countries-no_match.txt', 'w') as file:\n",
    "    # Write each element on a new line\n",
    "    for element in countries_not_mapped_1:\n",
    "        file.write(f\"{element}\\n\")\n",
    "    for element in countries_not_mapped_2:\n",
    "        file.write(f\"{element}\\n\")\n",
    "        \n",
    "# Write the updated data frame to a file\n",
    "df_quality.to_csv('wp_politicians_by_country.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3a8a9c",
   "metadata": {},
   "source": [
    "# Step 4: Analysis\n",
    "\n",
    "Now that we have constructed our data set (found in wp_politicians_by_country.csv), we will creating a series of tables in Step 5. The tables are all related to per_capita metrics but since our population data is provided in million, we are going to calculating the \"per 1 million people\" metric instead. And lastly, some of these tables involve the concept of \"high quality\" articles. In this project, we define high quality articles as those receiving an ORES score of \"FA\" (Featured Article) or \"GA\" (Good Article). Pandas data frames support a lot of table operations that will be helpful in calculating these metrics!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fda2e6",
   "metadata": {},
   "source": [
    "# Step 5: Results\n",
    "\n",
    "In this section, we will be creating 6 tables from the data we collected and cleaned in the prior steps. \n",
    "\n",
    "Table 1: The 10 countries with the highest total articles per capita (in descending order).\n",
    "\n",
    "Table 2: The 10 countries with the lowest total articles per capita (in ascending order).\n",
    "\n",
    "Table 3: The 10 countries with the highest high quality articles per capita (in descending order).\n",
    "\n",
    "Table 4: The 10 countries with the lowest high quality articles per capita (in ascending order).\n",
    "\n",
    "Table 5: A rank ordered list of geographic regions (in descending order) by total articles per capita.\n",
    "\n",
    "Table 6: Rank ordered list of geographic regions (in descending order) by high quality articles per capita."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91fe6f0",
   "metadata": {},
   "source": [
    "Read in the dataset we created in the prior steps and remove the rows without population data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8806e244",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('wp_politicians_by_country.csv')\n",
    "df_filtered = df.dropna(subset=['population'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b094c470",
   "metadata": {},
   "source": [
    "Create table 1 and table 2 (See Step 5 description for table details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd0fb38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 highest ranking countries by Articles per 1 Million People (Inf indicates that country population is significantly less than 1 million)\n",
      "                       country  articles_per_million\n",
      "                        Monaco                   inf\n",
      "                        Tuvalu                   inf\n",
      "           Antigua and Barbuda            330.000000\n",
      "Federated States of Micronesia            140.000000\n",
      "              Marshall Islands            130.000000\n",
      "                         Tonga            100.000000\n",
      "                      Barbados             83.333333\n",
      "                    Montenegro             60.000000\n",
      "                    Seychelles             60.000000\n",
      "                      Maldives             55.000000\n",
      "\n",
      "10 lowest ranking countries by Articles per 1 Million People\n",
      "      country  articles_per_million\n",
      "        China              0.011337\n",
      "        Ghana              0.087977\n",
      "        India              0.105698\n",
      " Saudi Arabia              0.135501\n",
      "       Zambia              0.148515\n",
      "       Norway              0.181818\n",
      "       Israel              0.204082\n",
      "        Egypt              0.304183\n",
      "Cote d'Ivoire              0.323625\n",
      "     Ethiopia              0.347826\n"
     ]
    }
   ],
   "source": [
    "# Group and collapse the data by country\n",
    "articles_per_country = df_filtered.groupby('country').size().reset_index(name='total_articles')\n",
    "articles_per_country = articles_per_country.merge(df_filtered[['country', 'population']].drop_duplicates(), on='country')\n",
    "\n",
    "# Calculate the articles per capita metric and make a new column for it\n",
    "articles_per_country['articles_per_million'] = articles_per_country['total_articles'] / articles_per_country['population']\n",
    "\n",
    "# Display table 1\n",
    "top_10_countries = articles_per_country.sort_values(by='articles_per_million', ascending=False).head(10)\n",
    "print(\"10 highest ranking countries by Articles per 1 Million People (Inf indicates that country population is significantly less than 1 million)\")\n",
    "print(top_10_countries[['country', 'articles_per_million']].to_string(index=False))\n",
    "\n",
    "print()\n",
    "\n",
    "# Display table 2 \n",
    "bottom_10_countries = articles_per_country.sort_values(by='articles_per_million', ascending=True).head(10)\n",
    "print(\"10 lowest ranking countries by Articles per 1 Million People\")\n",
    "print(bottom_10_countries[['country', 'articles_per_million']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e00a2b9",
   "metadata": {},
   "source": [
    "Create table 3 and table 4 (See Step 5 description for table details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a79ff75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 highest ranking countries by High Quality Articles per 1 Million People\n",
      "              country  high_quality_articles_per_million\n",
      "           Montenegro                           5.000000\n",
      "           Luxembourg                           2.857143\n",
      "              Albania                           2.592593\n",
      "               Kosovo                           2.352941\n",
      "             Maldives                           1.666667\n",
      "            Lithuania                           1.379310\n",
      "              Croatia                           1.315789\n",
      "               Guyana                           1.250000\n",
      "Palestinian Territory                           1.090909\n",
      "             Slovenia                           0.952381\n",
      "\n",
      "10 lowest ranking countries by Articles per 1 Million People\n",
      "   country  high_quality_articles_per_million\n",
      "Bangladesh                           0.005764\n",
      "     Egypt                           0.009506\n",
      "  Ethiopia                           0.015810\n",
      "     Japan                           0.016064\n",
      "  Pakistan                           0.016632\n",
      "  Colombia                           0.019157\n",
      "  Congo DR                           0.019550\n",
      "   Vietnam                           0.020222\n",
      "    Uganda                           0.020576\n",
      "   Algeria                           0.021368\n"
     ]
    }
   ],
   "source": [
    "# Filter the data to just the high quality articles\n",
    "high_quality_df = df_filtered[df_filtered['article_quality'].isin(['FA', 'GA'])]\n",
    "\n",
    "# Group and collapse the data by country.\n",
    "# Calculate the high quality articles per capita metric and make a new column for it.\n",
    "high_quality_per_country = high_quality_df.groupby('country').size().reset_index(name='high_quality_articles')\n",
    "high_quality_per_country = high_quality_per_country.merge(df_filtered[['country', 'population']].drop_duplicates(), on='country')\n",
    "high_quality_per_country['high_quality_articles_per_million'] = high_quality_per_country['high_quality_articles'] / high_quality_per_country['population']\n",
    "\n",
    "# Display table 3\n",
    "top_10_high_quality_countries = high_quality_per_country.sort_values(by='high_quality_articles_per_million', ascending=False).head(10)\n",
    "print(\"10 highest ranking countries by High Quality Articles per 1 Million People\")\n",
    "print(top_10_high_quality_countries[['country', 'high_quality_articles_per_million']].to_string(index=False))\n",
    "\n",
    "print()\n",
    "\n",
    "# Display table 4\n",
    "bottom_10_high_quality_countries = high_quality_per_country.sort_values(by='high_quality_articles_per_million', ascending=True).head(10)\n",
    "print(\"10 lowest ranking countries by Articles per 1 Million People\")\n",
    "print(bottom_10_high_quality_countries[['country', 'high_quality_articles_per_million']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b407b5bc",
   "metadata": {},
   "source": [
    "Create table 5 and table 6 (See Step 5 description for table details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6603c7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked ordering of Geographic Regions by Articles per 1 Million People\n",
      "         region  total_articles  population_in_millions  articles_per_million\n",
      "SOUTHERN EUROPE             795                   152.0              5.230263\n",
      "      CARIBBEAN             218                    44.0              4.954545\n",
      " WESTERN EUROPE             497                   199.0              2.497487\n",
      " EASTERN EUROPE             709                   285.0              2.487719\n",
      "   WESTERN ASIA             608                   299.0              2.033445\n",
      "NORTHERN EUROPE             190                   108.0              1.759259\n",
      "SOUTHERN AFRICA             123                    70.0              1.757143\n",
      "        OCEANIA              72                    45.0              1.600000\n",
      " EASTERN AFRICA             664                   483.0              1.374741\n",
      "  SOUTH AMERICA             569                   426.0              1.335681\n",
      "   CENTRAL ASIA             106                    80.0              1.325000\n",
      "NORTHERN AFRICA             301                   256.0              1.175781\n",
      " WESTERN AFRICA             513                   442.0              1.160633\n",
      "  MIDDLE AFRICA             230                   202.0              1.138614\n",
      "CENTRAL AMERICA             188                   182.0              1.032967\n",
      " SOUTHEAST ASIA             394                   682.0              0.577713\n",
      "     SOUTH ASIA             670                  2029.0              0.330212\n",
      "      EAST ASIA             152                  1648.0              0.092233\n",
      "\n",
      "Ranked ordering of Geographic Regions by High Quality Articles per 1 Million People\n",
      "         region  high_quality_articles  population_in_millions  high_quality_articles_per_million\n",
      "SOUTHERN EUROPE                     53                   152.0                           0.348684\n",
      "      CARIBBEAN                      9                    44.0                           0.204545\n",
      " EASTERN EUROPE                     38                   285.0                           0.133333\n",
      "SOUTHERN AFRICA                      8                    70.0                           0.114286\n",
      " WESTERN EUROPE                     21                   199.0                           0.105528\n",
      "   WESTERN ASIA                     27                   299.0                           0.090301\n",
      "NORTHERN EUROPE                      9                   108.0                           0.083333\n",
      "NORTHERN AFRICA                     17                   256.0                           0.066406\n",
      "   CENTRAL ASIA                      5                    80.0                           0.062500\n",
      "CENTRAL AMERICA                     10                   182.0                           0.054945\n",
      "  SOUTH AMERICA                     19                   426.0                           0.044601\n",
      "  MIDDLE AFRICA                      8                   202.0                           0.039604\n",
      " SOUTHEAST ASIA                     25                   682.0                           0.036657\n",
      " EASTERN AFRICA                     17                   483.0                           0.035197\n",
      " WESTERN AFRICA                     13                   442.0                           0.029412\n",
      "        OCEANIA                      1                    45.0                           0.022222\n",
      "     SOUTH ASIA                     21                  2029.0                           0.010350\n",
      "      EAST ASIA                      3                  1648.0                           0.001820\n"
     ]
    }
   ],
   "source": [
    "# Group the articles by region. Add up the article counts in each region using aggregation.\n",
    "articles_per_region = df_filtered.groupby('region').agg(\n",
    "    total_articles=('article_title', 'size')).reset_index()\n",
    "\n",
    "# Add in the region population data\n",
    "for index, row in articles_per_region.iterrows():\n",
    "    articles_per_region.loc[index, 'population_in_millions'] = int(region_map[row['region']][0])\n",
    "\n",
    "# Calculate articles per million\n",
    "articles_per_region['articles_per_million'] = articles_per_region['total_articles'] / articles_per_region['population_in_millions']\n",
    "\n",
    "# Sort\n",
    "articles_per_region_sorted = articles_per_region.sort_values(by='articles_per_million', ascending=False)\n",
    "\n",
    "# Display table 5\n",
    "print(\"Ranked ordering of Geographic Regions by Articles per 1 Million People\")\n",
    "print(articles_per_region_sorted[['region', 'total_articles', 'population_in_millions', 'articles_per_million']].to_string(index=False))\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "# Remove articles that are not \"high quality\"\n",
    "high_quality_filtered = df_filtered[df_filtered['article_quality'].isin(['FA', 'GA'])]\n",
    "\n",
    "# Group the articles by region. Add up the article counts in each region using aggregation.\n",
    "high_quality_per_region = high_quality_df.groupby('region').agg(\n",
    "    high_quality_articles=('article_title', 'size')).reset_index()\n",
    "\n",
    "# Add in the region population data\n",
    "for index, row in high_quality_per_region.iterrows():\n",
    "    high_quality_per_region.loc[index, 'population_in_millions'] = int(region_map[row['region']][0])\n",
    "\n",
    "# Calculate high-quality articles per million\n",
    "high_quality_per_region['high_quality_articles_per_million'] = high_quality_per_region['high_quality_articles'] / high_quality_per_region['population_in_millions']\n",
    "\n",
    "# Sort\n",
    "high_quality_per_region_sorted = high_quality_per_region.sort_values(by='high_quality_articles_per_million', ascending=False)\n",
    "\n",
    "# Display table 6\n",
    "print(\"Ranked ordering of Geographic Regions by High Quality Articles per 1 Million People\")\n",
    "print(high_quality_per_region_sorted[['region', 'high_quality_articles', 'population_in_millions', 'high_quality_articles_per_million']].to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
